# Stochastic Gradient Updates

##  Stochastic Gradient Updates
n 个样本，i 表示样本的 index，那么 objective function：
$$f(\mathbf{x}) = \frac{1}{n} \sum_{i = 1}^n f_i(\mathbf{x}).$$

x 处的 gradient of objective function：  
$$\nabla f(\mathbf{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\mathbf{x}).$$

如果每一轮 iteration 所有的样本 n 都参与 gradient descent 的计算，那么计算开销会很大，stochastic gradient descent (SGD) 的思想就是每一轮的迭代过程中，从总体样本随机抽取 n 个样本计算梯度，这样每一轮 iteration 的计算开销就从 O(n) 降低到 O(1)，其中 η 是学习率。
$$\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla f_i(\mathbf{x}),$$

并且，SGD的梯度是 full gradient 的无偏估计：
$$\mathbb{E}_ i \nabla f_ i(\mathbf{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\mathbf{x}) = \nabla f(\mathbf{x}).$$

向梯度中注入服从N~(0,1)分布的噪声，以便模拟 SGD 梯度的效果，下图显示的梯度轨迹会在最终处摇摆，原因是 SGD 的本质（噪声）导致的，该情况不会在多轮训练之后得到缓解，因此我们只能通过动态调整学习率的方式来改善这种现象：  

```python
def f(x1, x2):  # Objective function
    return x1**2 + 2 * x2**2

def f_grad(x1, x2):  # Gradient of the objective function
    return 2 * x1, 4 * x2

def sgd(x1, x2, s1, s2, f_grad):
    g1, g2 = f_grad(x1, x2)
    # Simulate noisy gradient
    g1 += torch.normal(0.0, 1, (1,))
    g2 += torch.normal(0.0, 1, (1,))
    eta_t = eta * lr()
    return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)
    
def constant_lr():
    return 1

eta = 0.1
lr = constant_lr  # Constant learning rate
d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))
```

![image](https://user-images.githubusercontent.com/44680953/144075815-1f46932b-901a-4c5d-87ea-7a28be09e09f.png)  

##  Dynamic Learning Rate

以下是随时间动态调节学习率的一些方法：  

$$\eta(t) & = \eta_i \text{ if } t_i \leq t \leq t_{i+1}  && \text{piecewise constant} $$
$$\eta(t) & = \eta_0 \cdot e^{-\lambda t} && \text{exponential decay} $$
$$\eta(t) & = \eta_0 \cdot (\beta t + 1)^{-\alpha} && \text{polynomial decay} $$

>  A popular choice is *polynomial decay* with $\alpha = 0.5$. In the case of convex optimization there are a number of proofs that show that this rate is well behaved. 

exponenial decay:  
```python
def exponential_lr():
    # Global variable that is defined outside this function and updated inside
    global t
    t += 1
    return math.exp(-0.1 * t)

t = 1
lr = exponential_lr
d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=1000, f_grad=f_grad))
```
![image](https://user-images.githubusercontent.com/44680953/144076560-dd7d2284-0f96-455c-b6da-3edef8349730.png)  

polynomial decay:  
> A popular choice is *polynomial decay* with $\alpha = 0.5$. In the case of convex optimization there are a number of proofs that show that this rate is well behaved. 
```python
def polynomial_lr():
    # Global variable that is defined outside this function and updated inside
    global t
    t += 1
    return (1 + 0.1 * t) ** (-0.5)

t = 1
lr = polynomial_lr
d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad))
```
![image](https://user-images.githubusercontent.com/44680953/144077313-603dfb80-3566-4100-a683-9d5c40355ad9.png)  
