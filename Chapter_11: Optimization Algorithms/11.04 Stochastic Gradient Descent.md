# Stochastic Gradient Updates

##  Stochastic Gradient Updates
n 个样本，i 表示样本的 index，那么 objective function：
$$f(\mathbf{x}) = \frac{1}{n} \sum_{i = 1}^n f_i(\mathbf{x}).$$

x 处的 gradient of objective function：  
$$\nabla f(\mathbf{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\mathbf{x}).$$

如果每一轮 iteration 所有的样本 n 都参与 gradient descent 的计算，那么计算开销会很大，stochastic gradient descent (SGD) 的思想就是每一轮的迭代过程中，从总体样本随机抽取 n 个样本计算梯度，这样每一轮 iteration 的计算开销就从 O(n) 降低到 O(1)，其中 η 是学习率。
$$\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla f_i(\mathbf{x}),$$

并且，SGD的梯度是 full gradient 的无偏估计：
$$\mathbb{E}_i \nabla f_i(\mathbf{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\mathbf{x}) = \nabla f(\mathbf{x}).$$
