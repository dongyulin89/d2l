Convexity 常作为一种重要的优化方法，如果在凸场景下算法算法表现得不好，那么在其他情况下往往会更差。

# Definitions

## Convex sets
A set $\mathcal{X}$ in a vector space is *convex* if for any $a, b \in \mathcal{X}$ the line segment connecting $a$ and $b$ is also in $\mathcal{X}$. In mathematical terms this means that for all $\lambda \in [0, 1]$ we have

$$\lambda  a + (1-\lambda)  b \in \mathcal{X} \text{ whenever } a, b \in \mathcal{X}.$$

![image](https://user-images.githubusercontent.com/44680953/143719825-b700c40f-cb89-4971-94a3-24cad498291b.png)   
向量空间中的某一个集合里面，任意两点组成的向量依然属于该集合，那么这个集合就是凸的。

![image](https://user-images.githubusercontent.com/44680953/143719920-a7492b6c-63a6-4082-be3b-a3f39c17d3dd.png)  
两个凸集的交集，仍然是一个凸集。

## Convex function
Given a convex set $\mathcal{X}$, a function $f: \mathcal{X} \to \mathbb{R}$ is *convex* if for all $x, x' \in \mathcal{X}$ and for all $\lambda \in [0, 1]$ we have

$$\lambda f(x) + (1-\lambda) f(x') \geq f(\lambda x + (1-\lambda) x').$$

![image](https://user-images.githubusercontent.com/44680953/143720091-668cb2e7-09c0-4c75-a25f-2c890a92733d.png)  
中间的余弦函数不是凸函数。

## Jensen’s Inequality
Given a convex function $f$, *Jensen's inequality*  作为 convexity 的一般化定义:

$$\sum_i \alpha_i f(x_i)  \geq f\left(\sum_i \alpha_i x_i\right)    \text{ and }    E_X[f(X)]  \geq f\left(E_X[X]\right),$$

where $\alpha_i$ are nonnegative real numbers such that $\sum_i \alpha_i = 1$ and $X$ is a random variable.

实际上，凸函数就是：函数之和 > 和的函数。

我们把 *Jensen's inequality* 作用于 log 函数：
$$E_{Y \sim P(Y)}[-\log P(X \mid Y)] \geq -\log P(X),$$

since $\int P(Y) P(X \mid Y) dY = P(X)$.
This can be used in variational methods. Here $Y$ is typically the unobserved random variable, $P(Y)$ is the best guess of how it might be distributed, and $P(X)$ is the distribution with $Y$ integrated out. For instance, in clustering $Y$ might be the cluster labels and $P(X \mid Y)$ is the generative model when applying cluster labels.

# Properties

## Local Minima Are Global Minima
注意，尽管 local minima = global minia，然而 global minia 仍然可能存在多个或者一个也不存在。

## Below Sets of Convex Functions Are Convex
凸函数的任意子区间任然是一个凸函数。

## Convexity and Second Derivatives
一个函数的 hassian matrix 是半正定（多维函数） 或者 二阶导数大于等于0（一维函数），则该函数是凸函数。

Formally, a twice-differentiable one-dimensional function $f: \mathbb{R} \rightarrow \mathbb{R}$ is convex
if and only if its second derivative $f'' \geq 0$. For any twice-differentiable multi-dimensional function $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$,
it is convex if and only if its Hessian $\nabla^2f \succeq 0$ (positive semidefinite), denoting the Hessian matrix $\nabla^2f$ by $\mathbf{H}$,
$\mathbf{x}^\top \mathbf{H} \mathbf{x} \geq 0$
for all $\mathbf{x} \in \mathbb{R}^n$.

# Constraints
凸函数的一个便利之处就是能够使用约束优化。
> One of the nice properties of convex optimization is that it allows us to handle constraints efficiently. That is, it allows us to solve *constrained optimization* problems of the form:

> $$\begin{aligned} \mathop{\mathrm{minimize~}}_{\mathbf{x}} & f(\mathbf{x}) \\
    \text{ subject to } & c_i(\mathbf{x}) \leq 0 \text{ for all } i \in \{1, \ldots, n\},
\end{aligned}$$

> where $f$ is the objective and the functions $c_i$ are constraint functions. To see what this does consider the case where $c_1(\mathbf{x}) = \|\mathbf{x}\|_2 - 1$. In this case the parameters $\mathbf{x}$ are constrained to the unit ball. If a second constraint is $c_2(\mathbf{x}) = \mathbf{v}^\top \mathbf{x} + b$, then this corresponds to all $\mathbf{x}$ lying on a half-space. Satisfying both constraints simultaneously amounts to selecting a slice of a ball.


## Lagrangian
其实就是利用拉格朗日乘数法（也叫拉格朗日乘子法）求解带有约束条件的多元函数极值问题。
> $$L(\mathbf{x}, \alpha_1, \ldots, \alpha_n) = f(\mathbf{x}) + \sum_{i=1}^n \alpha_i c_i(\mathbf{x}) \text{ where } \alpha_i \geq 0.$$

Here the variables $\alpha_i$ ($i=1,\ldots,n$) are the so-called *Lagrange multipliers* that ensure that constraints are properly enforced. They are chosen just large enough to ensure that $c_i(\mathbf{x}) \leq 0$ for all $i$. For instance, for any $\mathbf{x}$ where $c_i(\mathbf{x}) < 0$ naturally, we'd end up picking $\alpha_i = 0$. Moreover, this is a saddle point optimization problem where one wants to *maximize* $L$ with respect to all $\alpha_i$ and simultaneously *minimize* it with respect to $\mathbf{x}$. There is a rich body of literature explaining how to arrive at the function $L(\mathbf{x}, \alpha_1, \ldots, \alpha_n)$. For our purposes it is sufficient to know that the saddle point of $L$ is where the original constrained optimization problem is solved optimally.

## Penalties
采用添加惩罚项的方式进行约束。
> Consider weight decay in :numref:`sec_weight_decay`. In it we add $\frac{\lambda}{2} \|\mathbf{w}\|^2$ to the objective function to ensure that $\mathbf{w}$ does not grow too large. 

## Projections
采用映射的方式进行约束。

![image](https://user-images.githubusercontent.com/44680953/143866321-32f22214-6812-46e9-81fc-2909ae6bfb22.png)  
1，投影至球体表面：
$$ \mathbf{g} \leftarrow \mathbf{g} \cdot \mathrm{min}(1, \theta/\|\mathbf{g}\|) $$
2，投影至菱形表面：
$$\mathrm{Proj}_\mathcal{X}(\mathbf{x}) = \mathop{\mathrm{argmin}}_{\mathbf{x}' \in \mathcal{X}} \|\mathbf{x} - \mathbf{x}'\|,$$
