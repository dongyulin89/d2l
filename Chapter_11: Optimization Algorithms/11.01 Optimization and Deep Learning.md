# 优化目标
deep learning 中通常用 loss function 作为 optimization，其目标是找到一个合适的 model（有限数据量）使得 loss function 最小化；
optimization 中，通常把 loss function 称作 objective function，其目标是使 loss function 最小化（如遇最大化问题，取负号即可）；  

deep learning 的目标是找到一个合适的 model 使得 generalization error 最小化；  
optimization 的目标是找到一个合适的 objective function，使得 objective function 在有限的 training datasets 情况下，使得 training error 最小化；  
尽管二者目的不尽相同，我们常常把 deep learning 中的 loss function 作为 optimization 中的 objective function，使用 optimization 的优化手段进行 loss function 的优化；  

定义 empirical error 作为有限数据集情况下的 training error；  
定义 error 作为整个数据集的 expect loss（generalization error）；  

两者优化目标之间的差距，主要来讲就是 empirical risk 和 risk (generalization error) 之间的差距    
![image](https://user-images.githubusercontent.com/44680953/143712832-eecee9ec-9927-4074-a752-ba0785bb01d3.png)   

# 优化挑战
- Local Minima
- Saddle Points
- Vanishing Gradients  

总的来讲，以上三种情况都会导致 gradient vanish，从而使得 gradient descent 的方法不再有效。

## Local Minima
这里主要讲 local minimum 和 global minimum 之间的区别：  
![image](https://user-images.githubusercontent.com/44680953/143718875-5cdc3586-22fc-4e42-98d7-fab7a117d948.png)  
解决这种问题通常采用 minibatch stochastic gradient descent 的方法。  


## Saddle Points

## Vanishing Gradients
