# 优化目标
主要来讲就是 empirical risk 和 risk (generalization error) 之间的差距  
![image](https://user-images.githubusercontent.com/44680953/143712832-eecee9ec-9927-4074-a752-ba0785bb01d3.png)  

deep learning 中通常用 loss function 作为 optimization，其目标是找到一个合适的 model（有限数据量）使得 loss function 最小化；
optimization 中，通常把 loss function 称作 objective function，其目标是使 loss function 最小化（如遇最大化问题，取负号即可）；  

deep learning 的目标是找到一个合适的 model 使得 generalization error 最小化；  
optimization 的目标是找到一个合适的 objective function，使得 objective function 在有限的 training datasets 情况下，使得 training error 最小化；  
尽管二者目的不尽相同，我们常常把 deep learning 中的 loss function 作为 optimization 中的 objective function，使用 optimization 的优化手段进行 loss function 的优化；  

定义 empirical error 作为有限数据集情况下的 training error；  
定义 error 作为整个数据集的 expect loss（generalization error）；  

两者优化目标之间的差距，主要来讲就是 empirical risk 和 risk (generalization error) 之间的差距    
![image](https://user-images.githubusercontent.com/44680953/143712832-eecee9ec-9927-4074-a752-ba0785bb01d3.png) 
