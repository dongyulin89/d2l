# One-Dimensional Gradient Descent
一维梯度下降的公式推导：
> $$f(x + \epsilon) = f(x) + \epsilon f'(x) + \mathcal{O}(\epsilon^2).$$
> $$f(x - \eta f'(x)) = f(x) - \eta f'^2(x) + \mathcal{O}(\eta^2 f'^2(x)).$$
> $$f(x - \eta f'(x)) \lessapprox f(x).$$
> $$x \leftarrow x - \eta f'(x)$$

![image](https://user-images.githubusercontent.com/44680953/143867512-19d65084-adee-4f05-9937-93a8bc7cbbc3.png)  

## Learning Rate
学习率太小：梯度下降的幅度小
![image](https://user-images.githubusercontent.com/44680953/143867676-59a9ecb3-7ed3-4dfe-9d62-c404c849e419.png)  
学习率太大：梯度下降的幅度大
![image](https://user-images.githubusercontent.com/44680953/143867774-8ab363c2-8fca-475a-b5fb-07fd8d452f44.png)  


