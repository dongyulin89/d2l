# One-Dimensional Gradient Descent
一维梯度下降的公式推导：
$$f(x + \epsilon) = f(x) + \epsilon f'(x) + \mathcal{O}(\epsilon^2).$$
To keep things simple we pick a fixed step size $\eta > 0$ and choose $\epsilon = -\eta f'(x)$.
$$f(x - \eta f'(x)) = f(x) - \eta f'^2(x) + \mathcal{O}(\eta^2 f'^2(x)).$$
$$f(x - \eta f'(x)) \le f(x).$$
$$x \leftarrow x - \eta f'(x)$$

![image](https://user-images.githubusercontent.com/44680953/143867512-19d65084-adee-4f05-9937-93a8bc7cbbc3.png)  

## Learning Rate
学习率太小：梯度下降的幅度小  
![image](https://user-images.githubusercontent.com/44680953/143867676-59a9ecb3-7ed3-4dfe-9d62-c404c849e419.png)  
学习率太大：梯度下降的幅度大  
![image](https://user-images.githubusercontent.com/44680953/143867774-8ab363c2-8fca-475a-b5fb-07fd8d452f44.png)  


## Local Minima
对于 nonconvex function，采用梯度下降的方法就会陷入局部最小值的情况  
![image](https://user-images.githubusercontent.com/44680953/143868205-b880f743-8063-4094-8e02-b8f23f91d3fa.png)

# Multivariate Gradient Descent
多维梯度下降的公式推导：  
$$\nabla f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_d}\bigg]^\top.$$
$$f(\mathbf{x} + \boldsymbol{\epsilon}) = f(\mathbf{x}) + \mathbf{\boldsymbol{\epsilon}}^\top \nabla f(\mathbf{x}) + \mathcal{O}(\|\boldsymbol{\epsilon}\|^2).$$
$$\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla f(\mathbf{x}).$$

![image](https://user-images.githubusercontent.com/44680953/143868803-90016dd7-7e8e-4632-9172-61884c5b8d71.png)  


